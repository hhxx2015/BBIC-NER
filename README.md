# 医疗NER
# Bert+BiLSTM+CRF 

#### 一、该方法表现

训练数据：train.txt ; 测试数据：dev.txt

在dev.txt上的表现：

|                             方法                             | Precision | Recall |   F1   |
| :----------------------------------------------------------: | :-------: | :----: | :----: |
|             Bert+BiLSTM+CRF+嵌套实体的[MASK]处理             |     /     |   /    | ≤0.54  |
|     Bert+BiLSTM+CRF+嵌套实体的[MASK]处理+实体最小再扩展      |     /     |   /    | ≤0.58  |
|     BBIC+嵌套实体的[MASK]处理+实体最小再扩展+标注重验证      |  0.5912   | 0.5982 | 0.5946 |
| BBIC+嵌套实体的[MASK]处理+标注最小再扩展+标注重验证+SYM单独训练交叉验证 |  0.6095   | 0.6189 | 0.6142 |

#### 二、运行

大文件，以度盘示下：

1.bert_hrtp/bert_base中使用的是哈工大的中文预训练模型。（解压约1G）

链接：https://pan.baidu.com/s/1THK8orowuZVN0DUGzNcKYQ 提取码：ts3m 


2.models文件夹中

（1）all文件夹存放训练好的模型（解压约19G）



（2）output_sym文件夹存放训练好的模型（解压约19G）



<u>主要使用 **训练_run.py**、**测试_run.py**，再文件开头配置Bert文件、data文件夹、output文件夹\models文件夹、以及标签。然后run起来即可。（个人使用的是tensorflow1.14）</u>

#### 三、主要思路



通过 BERT 模型预处理生成基于上下文信息的词向量, 再将训练出来的词向量输入 BiLSTM-CRF 模型，利用CRF对BiLSTM输出结果进行解吗，得到一个预测标注序列，然后对序列中的各个实体提取分类。



![image-20210205062444974](img\image-20210205062444974.png)



![image-20210205064837972](img\image-20210205064837972.png)

-----



> 
>
> BERT模型沿袭了GPT模型的结构，采用Transfomer的编码器作为主体模型结构。Transformer舍弃了RNN的循环式网络结构，完全基于注意力机制来对一段文本进行建模。
>
> Transformer所使用的注意力机制的核心思想是去计算一句话中的每个词对于这句话中所有词的相互关系，然后认为这些词与词之间的相互关系在一定程度上反应了这句话中不同词之间的关联性以及重要程度。因此再利用这些相互关系来调整每个词的重要性（权重）就可以获得每个词新的表达。这个新的表征不但蕴含了该词本身，还蕴含了其他词与这个词的关系，因此和单纯的词向量相比是一个更加全局的表达。
>
> Transformer通过对输入的文本不断进行这样的注意力机制层和普通的非线性层交叠来得到最终的文本表达。
>
> ![image-20210205063455795](img\image-20210205063455795.png)
>
> 而双向LSTM基本思想就是对每个词序列分别采取前向和后向LSTM，再将同时刻输出进行合并。
>
> ![image-20210205063547760](img\image-20210205063547760.png)
>
> 最后处理相邻字符转移特征，加上CRF层得到标注结果。

over.



